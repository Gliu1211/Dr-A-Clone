{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import spacy\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    masking significant epistemological distinctio...\n",
      "1    For in - stance, when we use the term â€œcogniti...\n",
      "2    Similarly, when an ap - proach to learning and...\n",
      "3    All of the aforementioned theories and their i...\n",
      "4    Further, such epistemological differences make...\n",
      "Name: SENTENCE, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "csv_folder = \"./CSVS/\"\n",
    "csv_files = os.listdir(csv_folder)\n",
    "\n",
    "sentences_df = pd.DataFrame(columns=[\"TITLE\", \"SENTENCE\", \"WORD_COUNT\"])\n",
    "\n",
    "\n",
    "for file in csv_files:\n",
    "    path = os.path.join(csv_folder, file)\n",
    "    title = file.replace(\".csv\", \"\") \n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Check if the \"SENTENCES\" column exists in the DataFrame\n",
    "    if \"SENTENCES\" in df.columns:\n",
    "        # Create a new DataFrame with titles and sentences\n",
    "        temp_df = df[[\"SENTENCES\"]].rename(columns={\"SENTENCES\": \"SENTENCE\"})\n",
    "        temp_df[\"TITLE\"] = title  # Add the title column\n",
    "\n",
    "        # Append to the main sentences_df DataFrame\n",
    "        sentences_df = pd.concat([sentences_df, temp_df], ignore_index=True)\n",
    "\n",
    "print(sentences_df[\"SENTENCE\"].head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "matplotlib is required for plotting when the default backend \"matplotlib\" is selected.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m sentences_df[\u001b[39m\"\u001b[39m\u001b[39mSENTENCE\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m sentences_df[\u001b[39m\"\u001b[39m\u001b[39mSENTENCE\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39m\"\u001b[39m\u001b[39mstring\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m sentences_df[\u001b[39m\"\u001b[39m\u001b[39mWORD_COUNT\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m sentences_df[\u001b[39m\"\u001b[39m\u001b[39mSENTENCE\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39mlen\u001b[39m(x\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)))\n\u001b[1;32m----> 6\u001b[0m sentences_df[\u001b[39m\"\u001b[39;49m\u001b[39mWORD_COUNT\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mhist()\n",
      "File \u001b[1;32mc:\\Users\\akaru\\School\\Fall 2024 ML Research\\Dr. A Sentences\\Dr-A-Clone\\clean_env\\Lib\\site-packages\\pandas\\plotting\\_core.py:128\u001b[0m, in \u001b[0;36mhist_series\u001b[1;34m(self, by, ax, grid, xlabelsize, xrot, ylabelsize, yrot, figsize, bins, backend, legend, **kwargs)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhist_series\u001b[39m(\n\u001b[0;32m     48\u001b[0m     \u001b[39mself\u001b[39m: Series,\n\u001b[0;32m     49\u001b[0m     by\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m     61\u001b[0m ):\n\u001b[0;32m     62\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39m    Draw histogram of the input series using matplotlib.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[39m        >>> hist = ser.groupby(level=0).hist()\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 128\u001b[0m     plot_backend \u001b[39m=\u001b[39m _get_plot_backend(backend)\n\u001b[0;32m    129\u001b[0m     \u001b[39mreturn\u001b[39;00m plot_backend\u001b[39m.\u001b[39mhist_series(\n\u001b[0;32m    130\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m    131\u001b[0m         by\u001b[39m=\u001b[39mby,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    142\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\akaru\\School\\Fall 2024 ML Research\\Dr. A Sentences\\Dr-A-Clone\\clean_env\\Lib\\site-packages\\pandas\\plotting\\_core.py:1944\u001b[0m, in \u001b[0;36m_get_plot_backend\u001b[1;34m(backend)\u001b[0m\n\u001b[0;32m   1941\u001b[0m \u001b[39mif\u001b[39;00m backend_str \u001b[39min\u001b[39;00m _backends:\n\u001b[0;32m   1942\u001b[0m     \u001b[39mreturn\u001b[39;00m _backends[backend_str]\n\u001b[1;32m-> 1944\u001b[0m module \u001b[39m=\u001b[39m _load_backend(backend_str)\n\u001b[0;32m   1945\u001b[0m _backends[backend_str] \u001b[39m=\u001b[39m module\n\u001b[0;32m   1946\u001b[0m \u001b[39mreturn\u001b[39;00m module\n",
      "File \u001b[1;32mc:\\Users\\akaru\\School\\Fall 2024 ML Research\\Dr. A Sentences\\Dr-A-Clone\\clean_env\\Lib\\site-packages\\pandas\\plotting\\_core.py:1874\u001b[0m, in \u001b[0;36m_load_backend\u001b[1;34m(backend)\u001b[0m\n\u001b[0;32m   1872\u001b[0m         module \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39mimport_module(\u001b[39m\"\u001b[39m\u001b[39mpandas.plotting._matplotlib\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1873\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m-> 1874\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[0;32m   1875\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mmatplotlib is required for plotting when the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1876\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mdefault backend \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmatplotlib\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m is selected.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1877\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1878\u001b[0m     \u001b[39mreturn\u001b[39;00m module\n\u001b[0;32m   1880\u001b[0m found_backend \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: matplotlib is required for plotting when the default backend \"matplotlib\" is selected."
     ]
    }
   ],
   "source": [
    "# Add other features \n",
    "\n",
    "sentences_df[\"TITLE\"] = sentences_df[\"TITLE\"].astype(\"string\")\n",
    "sentences_df[\"SENTENCE\"] = sentences_df[\"SENTENCE\"].astype(\"string\")\n",
    "sentences_df[\"WORD_COUNT\"] = sentences_df[\"SENTENCE\"].apply(lambda x: len(x.split(\" \")))\n",
    "sentences_df[\"WORD_COUNT\"].hist()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Spacy to Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lastly, we foreground to students that the final exam would have a brief argumentative essay component, in which they would have to address an equally controversial point forwarded by Heather Wilson about the increasingly fragmented nature ofU.S (17, {'ADV': 3, 'PUNCT': 2, 'PRON': 3, 'VERB': 5, 'ADP': 4, 'NOUN': 6, 'SCONJ': 1, 'DET': 4, 'ADJ': 5, 'AUX': 2, 'PART': 1, 'PROPN': 2, 'NUM': 1}, ['ADV', 'PUNCT', 'PRON', 'VERB', 'ADP', 'NOUN', 'SCONJ', 'DET', 'ADJ', 'NOUN', 'AUX', 'VERB', 'DET', 'ADJ', 'ADJ', 'NOUN', 'NOUN', 'PUNCT', 'ADP', 'PRON', 'PRON', 'AUX', 'VERB', 'PART', 'VERB', 'DET', 'ADV', 'ADJ', 'NOUN', 'VERB', 'ADP', 'PROPN', 'PROPN', 'ADP', 'DET', 'ADV', 'ADJ', 'NOUN', 'NUM'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sentences = sentences_df[\"SENTENCE\"]\n",
    "random_sentence = np.random.choice(sentences)\n",
    "\n",
    "\n",
    "def getExtractedSentenceInfo(s: str):\n",
    "    doc = nlp(s)\n",
    "    is_stop_count = 0\n",
    "    pos_tag_count = collections.defaultdict(int)\n",
    "    structured_pos = []  # Store the POS tags in a structured way\n",
    "    deps = collections.defaultdict(set)\n",
    "\n",
    "    # Iterate over each token in the document\n",
    "    for token in doc:\n",
    "        if token.is_stop:\n",
    "            is_stop_count += 1\n",
    "        pos_tag_count[token.pos_] += 1\n",
    "        structured_pos.append((token.pos_))\n",
    "        dep = token.dep_\n",
    "        deps.add(token.text)\n",
    "\n",
    "    # Return both stop word count and POS tag dict\n",
    "    return is_stop_count, dict(pos_tag_count), structured_pos, dict(deps)\n",
    "\n",
    "\n",
    "print(random_sentence, getExtractedSentenceInfo(random_sentence))\n",
    "\n",
    "# Apply the function to the dataframe, storing both stop word counts and POS tags\n",
    "sentences_df[\"IS_STOP_COUNT\"], sentences_df[\"POS_TAGS\"], sentences_df[\"STRUCTURED_POS\"], sentences_df[\"DEPS\"] = zip(\n",
    "    *sentences_df[\"SENTENCE\"].apply(getExtractedSentenceInfo))\n",
    "\n",
    "# (1) Length (3 categorical variables) - Short, Medium, Long\n",
    "# (2) Stop Word [Complexity] - Low Count, High Count\n",
    "# (3) POS TAG RULES - Low Complexity (No SCONJs), Medium Complexity (1 SCONJ), High Complexity (2+ SCONJs)\n",
    "\n",
    "\n",
    "def getLengthCategory(word_count: int):\n",
    "    if word_count < 10:\n",
    "        return \"Short\"\n",
    "    elif word_count < 20:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Long\"\n",
    "\n",
    "\n",
    "def getStopCategory(stop_count: int, word_count: int):\n",
    "    # if fewer than 15% of the words are stop words, low, else high\n",
    "    # calculate the percentage of stop words\n",
    "    stop_percentage = stop_count / word_count\n",
    "\n",
    "    if stop_percentage < 0.15:\n",
    "        return \"Low\"\n",
    "    else:\n",
    "        return \"High\"\n",
    "\n",
    "\n",
    "def getPosCategory(pos_tags: dict):\n",
    "    # if there are more than 2 SCONJ tags, high, else medium, else low\n",
    "    if pos_tags.get(\"SCONJ\", 0) >= 2:\n",
    "        return \"High\"\n",
    "    elif pos_tags.get(\"SCONJ\", 0) == 1:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Low\"\n",
    "\n",
    "\n",
    "sentences_df[\"LENGTH_CATEGORY\"] = sentences_df[\"WORD_COUNT\"].apply(\n",
    "    getLengthCategory)\n",
    "sentences_df[\"STOP_CATEGORY\"] = sentences_df.apply(\n",
    "    lambda x: getStopCategory(x[\"IS_STOP_COUNT\"], x[\"WORD_COUNT\"]), axis=1)\n",
    "sentences_df[\"POS_CATEGORY\"] = sentences_df[\"POS_TAGS\"].apply(getPosCategory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4801"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"EXTRACTED_SENTENCE_DATA.csv\"\n",
    "sentences_df.to_csv(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(f'{path}')\n",
    "df_test.head()\n",
    "\n",
    "df_test[\"STOP_PERCENTAGE\"] = df_test[\"IS_STOP_COUNT\"]/ df_test[\"WORD_COUNT\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifications for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Length (3 categorical variables) - Short, Medium, Long\n",
    "# (2) Stop Word [Complexity] - Low Count, High Count\n",
    "# (3) POS TAG RULES - Low Complexity (No SCONJs), Medium Complexity (1 SCONJ), High Complexity (2+ SCONJs)\n",
    "\n",
    "def getLengthCategory(word_count: int):\n",
    "    if word_count < 10:\n",
    "        return \"Short\"\n",
    "    elif word_count < 20:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Long\"\n",
    "    \n",
    "def getStopCategory(stop_count: int , word_count: int):\n",
    "    # if fewer than 15% of the words are stop words, low, else high\n",
    "    # calculate the percentage of stop words\n",
    "    stop_percentage = stop_count / word_count\n",
    "\n",
    "    if stop_percentage < 0.15:\n",
    "        return \"Low\"\n",
    "    else:\n",
    "        return \"High\"\n",
    "    \n",
    "def getPosCategory(pos_tags: dict):\n",
    "    # if there are more than 2 SCONJ tags, high, else medium\n",
    "    if pos_tags.get(\"SCONJ\", 0) >= 2:\n",
    "        return \"High\"\n",
    "    elif pos_tags.get(\"SCONJ\", 0) == 1:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Low\"\n",
    "    \n",
    "    \n",
    "# loop through the dataframe and apply the functions\n",
    "sentences_df[\"LENGTH_CATEGORY\"] =sentences_df[\"WORD_COUNT\"].apply(getLengthCategory)\n",
    "sentences_df[\"STOP_CATEGORY\"] = sentences_df.apply(lambda x: getStopCategory(x[\"IS_STOP_COUNT\"], x[\"WORD_COUNT\"]), axis=1)\n",
    "sentences_df[\"POS_CATEGORY\"] = sentences_df[\"POS_TAGS\"].apply(getPosCategory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: NOUN SUBJECT AND NOUN OBJECTS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
